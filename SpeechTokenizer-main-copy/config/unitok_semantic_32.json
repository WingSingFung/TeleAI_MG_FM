{
    "training_stage": 1,
    "vae_ckpt": "/gemini/platform/public/aigc/mah_1/AIGCtools_audio/vae1d_25hz_64chs_audioset_0.187",
    "semantic_ckpt": "/gemini/platform/public/aigc/mah_1/mah/unitok/SpeechTokenizer-main/exps_semantic/12_5hz_asr_1018_tinyllama_whisper/logs/version_2/checkpoints/last.ckpt",
    "vocoder_ckpt": "./third_parties/checkpoints/bigvgan_audioset.ckpt",
    "stft_cfg": {
        "sampling_rate": 32000,
        "filter_length": 2048,
        "hop_length": 640,
        "win_length": 2048,
        "n_mel_channels": 128,
        "mel_fmin": 0,
        "mel_fmax": 16000
    },
    "vae_cfg": {
        "embed_dim": 64,
        "ddconfig": {
            "double_z": true,
            "in_channels": 128,
            "out_ch": 128,
            "z_channels": 64,
            "kernel_size": 5,
            "ch": 384,
            "ch_mult": [
                1,
                2,
                4
            ],
            "num_res_blocks": 4,
            "attn_layers": [
                3
            ],
            "down_layers": [
                0
            ],
            "dropout": 0.0
        }
    },

    "semantic_quantizer_cfg": {
        "dim": 1280,
        "codebook_size": 16384,
        "channel_first": false,
        "rotation_trick": false,
        "frozen_codebook_dim": 1280,
        "input_to_quantize_commit_loss_weight": 0.25,
        "commitment_weight": 1.0
    },

    "vocoder_cfg": {
        "upsample_rates": [8,5,2,2,2,2],
        "upsample_kernel_sizes": [16,10,4,4,4,4],
        "upsample_initial_channel": 1536,
        "resblock_kernel_sizes": [3,7,11],
        "resblock_dilation_sizes": [[1,3,5], [1,3,5], [1,3,5]],
        "use_tanh_at_final": false,
        "use_bias_at_final": false,
        "activation": "snakebeta",
        "snake_logscale": true,
        "resblock": "1",
        "num_mels": 128,
        "use_cuda_kernel": true
    },

    "acoustic_cfg":{
        "dim": 1024,
        "dim_inner": 3072,
        "mel_dim": 128,
        "depth": 12,
        "heads": 16,
        "dim_head": 64,
        "model_type": "25hz"
    },
    "codebook_size": 65536,
    "quantizer_cfg": {
        "e_dim": 512,
        "n_e": 65536,
        "beta": 0.25,
        "legacy": false
    },

    "loss_scale_quantizer" : 100.0,

    "decoder_cfg":{
        "dim": 1024,
        "dim_inner": 3072,
        "latent_dim": 64,
        "cond_dim": 512,
        "depth": 24,
        "heads": 16,
        "dim_head": 64,
        "model_type": "vae"
    },

    "cfg_drop_rate": 0.1,
    "cfg_guidance_scale": 1.5,
    "train_files": "/gemini/platform/public/aigc/mah_1/mah/unitok/SpeechTokenizer-main/data_processing/libritts_raw_data.json",
    "valid_files": "/gemini/platform/public/aigc/mah_1/mah/unitok/SpeechTokenizer-main/data_processing/libritts_raw_data.json",
    "results_folder": "exps_semanticgen_dit/fsq_25hz_medium_1022",
    "padding_mode": "repeat",
    "chunk_mode": "random", 
    "sample_rate": 32000,
    "batch_size": 32,
    "epochs":100,
    "learning_rate": 1e-4,
    "initial_learning_rate": 0,
    "num_warmup_steps": 0,
    "betas":[0.9, 0.95],
    "seed": 1234,
    "segment_size": 320000,
    "wd": 1e-2,
    "eps": 1e-6,
    "num_workers": 8,
    "num_ckpt_keep": 3,
    "showpiece_num": 8,
    "accelerator": "gpu",
    "devices": [0,1,2,3,4,5],
    "strategy": "ddp",
    "precision": "16-mixed"
}